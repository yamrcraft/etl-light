
spark {
  app-name = "kafka-etl"
  config {
    // Apache Spark configurations
    spark.executor.memory = "1g"
    spark.eventLog.enabled = "false"
    spark.ui.port = "4045"
  }
}

kafka {
  // list of topics to consume
  topics = ["events"]

  // Kafka configurations
  config {
    group.id = "kafka-etl"
    metadata.broker.list = "kafka:9092"
    auto.offset.reset = "smallest"
  }
}

etl {

  // distributed lock, use it to control concurrent runs.
  lock {
    enabled = false
    zookeeper-connect = "locahost:2181"
    path = "/etl/file.lock"
    wait-for-lock-seconds = 10
  }

  state {
    folder = "file:///var/etl/state"
    files-to-keep = 20
  }

  errors-folder = "file:///var/etl/errors/events"

  //max-num-of-output-files = 1

  pipeline {

    factory-class = "examples.protobuf.ProtoToParquetPipelineFactory"

    transformer = {
      config = {
        timestamp-field = "ts"
        timestamp-field-format = "yyyy-MM-dd HH:mm:ss"

        //default-message-type = "Default"
        //message-type-selection-field = "type"
      }
    }

    writer = {
      config = {
        // uses local file-system file to append data during processing
        working-folder = "file:///var/etl/work/events"

        // final destination folder - file will be copied from working-folder to output-folder on commit.
        output-folder = "file:///var/etl/output/events"

        // output folder structure (according to time retrieved from input event as specified by transformer.timestamp-field)
        partition {
          pattern = "YYYY/MM/dd/HH"
        }

        // mapping of record name (e.g. Avro record name) to folder name.
        record-name-to-folder-mapping {
          UserEvent = "users_events"
        }
      }
    }
  }

}
